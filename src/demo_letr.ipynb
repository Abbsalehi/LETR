{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "metadata": {
    "interpreter": {
     "hash": "410c54daf323f5212ce889dbd0c7b13970b5bad95aeecb5b05a0f5b22af8bc3f"
    }
   },
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LETR Basic Usage Demo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T21:04:28.898416Z",
     "start_time": "2024-07-05T21:04:28.293007Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "import torchvision.transforms.functional as functional\n",
    "import torch.nn.functional as F\n",
    "from models import build_model\n",
    "from util.misc import nested_tensor_from_tensor_list"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-07-05T21:04:29.399379Z",
     "start_time": "2024-07-05T21:04:29.390629Z"
    }
   },
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += \"    {0}\".format(t)\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = functional.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, img):\n",
    "        return functional.to_tensor(img)\n",
    "\n",
    "def resize(image, size, max_size=None):\n",
    "    # size can be min_size (scalar) or (w, h) tuple\n",
    "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
    "        w, h = image_size\n",
    "        if max_size is not None:\n",
    "            min_original_size = float(min((w, h)))\n",
    "            max_original_size = float(max((w, h)))\n",
    "            if max_original_size / min_original_size * size > max_size:\n",
    "                size = int(round(max_size * min_original_size / max_original_size))\n",
    "        if (w <= h and w == size) or (h <= w and h == size):\n",
    "            return (h, w)\n",
    "        if w < h:\n",
    "            ow = size\n",
    "            oh = int(size * h / w)\n",
    "        else:\n",
    "            oh = size\n",
    "            ow = int(size * w / h)\n",
    "        return (oh, ow)\n",
    "\n",
    "    def get_size(image_size, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            return size[::-1]\n",
    "        else:\n",
    "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
    "\n",
    "    size = get_size(image.size, size, max_size)\n",
    "    rescaled_image = functional.resize(image, size)\n",
    "\n",
    "    return rescaled_image\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, sizes, max_size=None):\n",
    "        assert isinstance(sizes, (list, tuple))\n",
    "        self.sizes = sizes\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        size = self.sizes\n",
    "        return resize(img, size, self.max_size)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "source": [
    "## Load Model Pre-trained Weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# obtain checkpoints\n",
    "checkpoint = torch.load('../exp/res101_stage2_focal/checkpoints/checkpoint0024.pth', map_location='cpu')\n",
    "\n",
    "# load model\n",
    "args = checkpoint['args']\n",
    "model, _, postprocessors = build_model(args)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Demo Image"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load image\n",
    "raw_img = plt.imread('../figures/demo.png')\n",
    "h, w = raw_img.shape[0], raw_img.shape[1]\n",
    "orig_size = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "# normalize image\n",
    "test_size = 1100\n",
    "normalize = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.538, 0.494, 0.453], [0.257, 0.263, 0.273]),\n",
    "        Resize([test_size]),\n",
    "    ])\n",
    "img = normalize(raw_img)\n",
    "inputs = nested_tensor_from_tensor_list([img])\n",
    "plt.axis('off')\n",
    "plt.imshow(raw_img)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Inference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "outputs = model(inputs)[0]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Post-processing Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out_logits, out_line = outputs['pred_logits'], outputs['pred_lines']\n",
    "prob = F.softmax(out_logits, -1)\n",
    "scores, labels = prob[..., :-1].max(-1)\n",
    "img_h, img_w = orig_size.unbind(0)\n",
    "scale_fct = torch.unsqueeze(torch.stack([img_w, img_h, img_w, img_h], dim=0), dim=0)\n",
    "lines = out_line * scale_fct[:, None, :]\n",
    "lines = lines.view(1000, 2, 2)\n",
    "lines = lines.flip([-1])# this is yxyx format\n",
    "scores = scores.detach().numpy()\n",
    "keep = scores >= 0.7\n",
    "keep = keep.squeeze()\n",
    "lines = lines[keep]\n",
    "lines = lines.reshape(lines.shape[0], -1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot Inference Results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(raw_img)\n",
    "lines = lines.detach().numpy()\n",
    "for tp_id, line in enumerate(lines):\n",
    "    y1, x1, y2, x2 = line # this is yxyx\n",
    "    p1 = (x1, y1)\n",
    "    p2 = (x2, y2)\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], linewidth=1.5, color='darkorange', zorder=1)\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "#plt.savefig(\"../figures/demo_result.png\", dpi=300, bbox_inches='tight', pad_inches = 0)\n",
    "#plt.close(fig)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ]
}
